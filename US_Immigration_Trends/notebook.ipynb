{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "* The purpose of this project is to understand the immigration trends using a data pipeline created using Spark and Parquet file format\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries, packages and modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import col, isnan, when, trim\n",
    "from pyspark.sql.functions import *\n",
    "from Load_Source import LoadSource\n",
    "from Clean_Source import CleanSource\n",
    "from Transform_Source import TransformSource\n",
    "from Model_Source import ModelSource\n",
    "from Build_Pipeline import BuildPipeline\n",
    "from Check_Validity import CheckValidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Spark session initialize\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Project Scope and Data Gathering\n",
    "* I have used the following 3 data sources:\n",
    " * I94 Immigration \n",
    " * U.S. City Demographic\n",
    " * Airport Code Table\n",
    "* I used the above data sources to create a data model used to understand immigration patterns in the US. \n",
    "* Tool/Framework used was Spark in parquet file format to extract, transform and build data model used to understand immigration patterns. Spark is used because of the ability to process massive datasets and parquet file format for efficient querying due to columnar storage format.\n",
    "\n",
    "#### Data Description\n",
    "* Following is the brief description of where the data sources come and what information does it include:\n",
    " * I94 Immigration Data: This data comes from the US National Tourism and Trade Office. The data contains international\n",
    "   visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of\n",
    "   transportation, age groups, states visited (first intended address only), and the top ports of entry (for select \n",
    "   countries)\n",
    " * U.S. City Demographic Data: This data comes from OpenSoft. This dataset contains information about the demographics of\n",
    "    all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US \n",
    "    Census Bureau's 2015 American Community Survey.\n",
    " * Airport Code Table: This is a simple table of airport codes and corresponding cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Refer the Load_Source.py file for related class and methods to load sas data and different csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "SrcLd1 = LoadSource(\"input_sources/sas_data\",spark)\n",
    "immi_df  = SrcLd1.load_sas_data()\n",
    "# Cache data\n",
    "immi_df.cache()\n",
    "\n",
    "SrcLd2 = LoadSource(\"input_sources/us-cities-demographics.csv\",spark)\n",
    "demog_df  = SrcLd2.load_csv_data()\n",
    "\n",
    "SrcLd3 = LoadSource(\"input_sources/airport-codes_csv.csv\",spark)\n",
    "airport_df  = SrcLd3.load_csv_data(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "|5748522.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20579.0|  57.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1959.0|10292016|     M|  null|     NZ|9.498180283E10|00010|      B2|\n",
      "|5748523.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  66.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1950.0|10292016|     F|  null|     NZ|9.497968993E10|00010|      B2|\n",
      "|5748524.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  41.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1975.0|10292016|     F|  null|     NZ|9.497974673E10|00010|      B2|\n",
      "|5748525.0|2016.0|   4.0| 245.0| 464.0|    HOU|20574.0|    1.0|     FL|20581.0|  27.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1989.0|10292016|     M|  null|     NZ|9.497324663E10|00028|      B2|\n",
      "|5748526.0|2016.0|   4.0| 245.0| 464.0|    LOS|20574.0|    1.0|     CA|20581.0|  26.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1990.0|10292016|     F|  null|     NZ|9.501354793E10|00002|      B2|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immi_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...| 1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...|11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian|32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino| 2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian|11060|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demog_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|     US-OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|     US-AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|     US-CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|     US-CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|     US-CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Data Cleaning Steps\n",
    "* Following are the steps taken in order to clean the data\n",
    "* Drop missing values\n",
    "* Drop NAN\n",
    "* Select Unique records\n",
    "* Rename specific columns\n",
    "* Select specific columns for each data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Refer the Clean_Source.py file for related class and methods to clean various source files using the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in Airport-Codes table before cleaning is 55075\n",
      "Total records in Airport-Codes table after cleaning is 28221\n",
      "Total records in SAS-immigration table before cleaning is 3096313\n",
      "Total records in SAS-immigration table after cleaning is 2431703\n",
      "Total records in US-demographics table before cleaning is 2891\n",
      "Total records in US-demographics table after cleaning is 2875\n"
     ]
    }
   ],
   "source": [
    "clean = CleanSource(airport_df,immi_df,demog_df)\n",
    "airport_clean = clean.clean_airport()\n",
    "immi_clean = clean.clean_immigration()\n",
    "immi_clean.cache()\n",
    "demog_clean = clean.clean_demographics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+------+--------+--------+\n",
      "| cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|gender|visatype|dtadfile|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+------+--------+--------+\n",
      "| 258.0|2016.0|   4.0| 103.0| 103.0|    NYC|20545.0|    1.0|     NY|20554.0|  55.0|    2.0|     M|      WT|20160401|\n",
      "| 569.0|2016.0|   4.0| 103.0| 103.0|    SFR|20545.0|    1.0|     CA|20562.0|  28.0|    2.0|     M|      WT|20160401|\n",
      "|1080.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     NY|20552.0|  18.0|    2.0|     F|      WT|20160401|\n",
      "|1393.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20550.0|  15.0|    2.0|     F|      WT|20160401|\n",
      "|1417.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20551.0|  47.0|    2.0|     F|      WT|20160401|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immi_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------+--------------------+------+\n",
      "|         City|     State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|state_code|                Race| Count|\n",
      "+-------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------+--------------------+------+\n",
      "|      Phoenix|   Arizona|      33.8|         786833|           776168|         1563001|             72388|      300702|        AZ|  Hispanic or Latino|669914|\n",
      "|   Round Rock|     Texas|      34.9|          56646|            59193|          115839|              6804|       18237|        TX|Black or African-...| 13636|\n",
      "|   Union City|California|      38.5|          38599|            35911|           74510|              1440|       32752|        CA|Black or African-...|  5508|\n",
      "|Santa Clarita|California|      38.1|          90192|            92175|          182367|              8537|       40666|        CA|American Indian a...|  4441|\n",
      "|       Toledo|      Ohio|      36.1|         135455|           144323|          279778|             15286|        9257|        OH|  Hispanic or Latino| 23614|\n",
      "+-------------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demog_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+----------+--------------------+\n",
      "| 00IS|small_airport|Hayenga's Cant Fi...|         820|       NA|         US|     US-IL|      00IS|-89.1229019165039...|\n",
      "| 03ID|small_airport|Flying Y Ranch Ai...|        3180|       NA|         US|     US-ID|      03ID|-116.532997131347...|\n",
      "| 09NY|     heliport|Spring Lake Fire ...|         254|       NA|         US|     US-NY|      09NY|-74.0488967895507...|\n",
      "|  0L4|small_airport|Lida Junction Air...|        4684|       NA|         US|     US-NV|       0L4|-117.191001892089...|\n",
      "| 16TS|small_airport|  Pineridge STOLport|         420|       NA|         US|     US-TX|      16TS|-95.3180007934570...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "* Used the following star schema\n",
    "* Fact Table\n",
    " * immi_fact (**i94addr,i94port,visa_code,visa_mode**, cicid, i94cit, i94res, arrdate,  depdate, i94bir, gender, visatype, dtadfile,  arrival_yr, arrival_mon)\n",
    "* Dimension Tables\n",
    "    * demog_dim (**state_code**,race,male_pop,female_pop,total_pop,foreign_born_pop,veteran_pop)\n",
    "    * airport_dim (**local_code**,ident,type,name,elevation_ft,continent,iso_country,iso_region,coordinates)\n",
    "* visa_code - mapped i94visa column into corresponding values using I94_SAS_Labels_Descriptions file\n",
    "* visa_mode - mapped i94mode into corresponding values using I94_SAS_Labels_Descriptions file\n",
    "\n",
    "\n",
    "* Reason for the model choice:\n",
    " * The star schema separates business process data into facts, and measurable descriptive attributes related to the fact\n",
    "   table into dimension tables\n",
    " * I wanted to analyze the following trends using the above data model:\n",
    "     * Determine male vs female count arriving at different airport types in US per month? (Use immi_fact and airport_dim)\n",
    "     * Determine which airport ports have the maximum number of immigrants arrival per month? (Use immi_fact and airport_dim)\n",
    "     * Determine trends between foreign_born population and the number of immigrants arrival per US state? \n",
    "       (Use immi_fact and demog_dim)\n",
    "     * Determine trends between male_population and number of men arrived in US per state? (Use immi_fact and demog_dim)\n",
    "     * Determine trends between female_population and number of females arrived in US per state? (Use immi_fact and demog_dim)\n",
    " * Thus by using the above star schema model, based on the above analysis we can determine key trends and immigration\n",
    "   activity in US which can guide key business decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "* Following are the steps taken to pipeline the data\n",
    "* Transformations\n",
    "   1. Transform the clean US demographics data to Group by state_code and race to get corresponding metrics\n",
    "   2. Transform the i94visa into the following categories of visa codes as per I94_SAS_Labels_Descriptions file:\n",
    "        * 1 = Business\n",
    "        * 2 = Pleasure\n",
    "        * 3 = Student\n",
    "   3. Transform the i94mode into the following categories of visa modes as per I94_SAS_Labels_Descriptions file:\n",
    "        * 1 = 'Air'\n",
    "        * 2 = 'Sea'\n",
    "        * 3 = 'Land'\n",
    "        * 9 = 'Not reported'          \n",
    "* Model Data \n",
    "   1. Model fact table using primary key and foreign key relation\n",
    "   2. Generate fact table\n",
    "   3. Generate dimensions table   \n",
    "* Build Pipelines\n",
    "   1. Generate immigration fact table in parquet file format partitioned by \"i94addr\",\"arrival_yr\",\"arrival_mon\"\n",
    "   2. Generate demographics dimensions table in parquet file format partitioned by \"state_code\",\"race\"\n",
    "   3. Genrate airport dimensions table in parquet file format partitioned by \"local_code\",\"iso_region\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Refer the Transform_Source.py file for related class and methods to transform source files using the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+----------+---------+----------------+-----------+\n",
      "|state_code|                race|male_pop|female_pop|total_pop|foreign_born_pop|veteran_pop|\n",
      "+----------+--------------------+--------+----------+---------+----------------+-----------+\n",
      "|        NJ|American Indian a...|  600089|    615303|  1215392|          419638|      25852|\n",
      "|        KY|American Indian a...|  452483|    477394|   929877|           66488|      56025|\n",
      "|        OK|Black or African-...|  714573|    734422|  1448995|          151174|      95468|\n",
      "|        CA|American Indian a...|12019895|  12288108| 24308003|         7310648|     909205|\n",
      "|        NH|               White|   97771|    100427|   198198|           27199|      11005|\n",
      "+----------+--------------------+--------+----------+---------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+------+------+-------+-------+-------+-------+------+------+--------+--------+---------+---------+----------+-----------+\n",
      "| cicid|i94cit|i94res|i94port|arrdate|i94addr|depdate|i94bir|gender|visatype|dtadfile|visa_code|visa_mode|arrival_yr|arrival_mon|\n",
      "+------+------+------+-------+-------+-------+-------+------+------+--------+--------+---------+---------+----------+-----------+\n",
      "| 258.0| 103.0| 103.0|    NYC|20545.0|     NY|20554.0|  55.0|     M|      WT|20160401| Pleasure|      Air|      2016|          4|\n",
      "| 569.0| 103.0| 103.0|    SFR|20545.0|     CA|20562.0|  28.0|     M|      WT|20160401| Pleasure|      Air|      2016|          4|\n",
      "|1080.0| 104.0| 104.0|    NEW|20545.0|     NY|20552.0|  18.0|     F|      WT|20160401| Pleasure|      Air|      2016|          4|\n",
      "|1393.0| 104.0| 104.0|    NYC|20545.0|     NY|20550.0|  15.0|     F|      WT|20160401| Pleasure|      Air|      2016|          4|\n",
      "|1417.0| 104.0| 104.0|    NYC|20545.0|     NY|20551.0|  47.0|     F|      WT|20160401| Pleasure|      Air|      2016|          4|\n",
      "+------+------+------+-------+-------+-------+-------+------+------+--------+--------+---------+---------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform Data\n",
    "transformer = TransformSource(immi_clean,demog_clean,airport_clean)\n",
    "demog_trans = transformer.transform_demog()\n",
    "immi_trans = transformer.transform_immi()\n",
    "demog_trans.show(5)\n",
    "immi_trans.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Refer the Model_Source.py file for related class and methods to model source files using the above steps into a star schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = ModelSource(immi_trans,demog_trans,airport_clean)\n",
    "immi_fact,demog_dim,airport_dim = model.star_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+-------+-------+-------+------+------+--------+--------+---------+---------+----------+-----------+\n",
      "| cicid|i94cit|i94res|i94port|arrdate|i94addr|depdate|i94bir|gender|visatype|dtadfile|visa_code|visa_mode|arrival_yr|arrival_mon|\n",
      "+------+------+------+-------+-------+-------+-------+------+------+--------+--------+---------+---------+----------+-----------+\n",
      "| 569.0| 103.0| 103.0|    SFR|20545.0|     CA|20562.0|  28.0|     M|      WT|20160401| Pleasure|      Air|      2016|          4|\n",
      "|1080.0| 104.0| 104.0|    NEW|20545.0|     NY|20552.0|  18.0|     F|      WT|20160401| Pleasure|      Air|      2016|          4|\n",
      "|1793.0| 104.0| 104.0|    BLA|20545.0|     CA|20551.0|  26.0|     M|      WT|20160401| Pleasure|     Land|      2016|          4|\n",
      "|3000.0| 108.0| 108.0|    NEW|20545.0|     NY|20553.0|  26.0|     F|      WT|20160401| Pleasure|      Air|      2016|          4|\n",
      "|3474.0| 108.0| 108.0|    SFR|20545.0|     AZ|20553.0|  13.0|     M|      WT|20160401| Pleasure|      Air|      2016|          4|\n",
      "+------+------+------+-------+-------+-------+-------+------+------+--------+--------+---------+---------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immi_fact.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+----------+---------+----------------+-----------+\n",
      "|state_code|                race|male_pop|female_pop|total_pop|foreign_born_pop|veteran_pop|\n",
      "+----------+--------------------+--------+----------+---------+----------------+-----------+\n",
      "|        NJ|American Indian a...|  600089|    615303|  1215392|          419638|      25852|\n",
      "|        KY|American Indian a...|  452483|    477394|   929877|           66488|      56025|\n",
      "|        OK|Black or African-...|  714573|    734422|  1448995|          151174|      95468|\n",
      "|        CA|American Indian a...|12019895|  12288108| 24308003|         7310648|     909205|\n",
      "|        NH|               White|   97771|    100427|   198198|           27199|      11005|\n",
      "+----------+--------------------+--------+----------+---------+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demog_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+----------+--------------------+\n",
      "| 00IS|small_airport|Hayenga's Cant Fi...|         820|       NA|         US|     US-IL|      00IS|-89.1229019165039...|\n",
      "| 03ID|small_airport|Flying Y Ranch Ai...|        3180|       NA|         US|     US-ID|      03ID|-116.532997131347...|\n",
      "| 09NY|     heliport|Spring Lake Fire ...|         254|       NA|         US|     US-NY|      09NY|-74.0488967895507...|\n",
      "|  0L4|small_airport|Lida Junction Air...|        4684|       NA|         US|     US-NV|       0L4|-117.191001892089...|\n",
      "| 16TS|small_airport|  Pineridge STOLport|         420|       NA|         US|     US-TX|      16TS|-95.3180007934570...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Refer the Build_Pipeline.py file for related class and methods to generate output files using the above mentioned steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin:writing Immigration Fact table to location /output_stg\n",
      "End:File written to /output_stg\n",
      "Begin:writing Demog Dimension table to location /output_stg\n",
      "End:File written to /output_stg\n",
      "Begin:writing Airport Dimension table to location /output_stg\n",
      "End:File written to /output_stg\n"
     ]
    }
   ],
   "source": [
    "build = BuildPipeline(immi_fact,demog_dim,airport_dim,\"/output_stg\")\n",
    "build.gen_immi_fact()\n",
    "build.gen_demog_dim()\n",
    "build.gen_airport_dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Refer the Check_Validity.py file for related class and methods to check validity of files using the above mentioned steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct records in demog_dim is 240\n",
      "Distinct state_code values in demog_dim Table is 48\n",
      "Checking for unique records\n",
      "Checking if primary key is not null\n",
      "Table check complete\n",
      "Distinct records in airport_dim is 28221\n",
      "Distinct local_code values in airport_dim Table is 26999\n",
      "Checking for unique records\n",
      "Checking if primary key is not null\n",
      "Table check complete\n",
      "Distinct records in immi_fact is 1536265\n",
      "Distinct i94addr values in immi_fact Table is 48\n",
      "Checking for unique records\n",
      "Checking if primary key is not null\n",
      "Table check complete\n",
      "Checking integrity constraints\n",
      "Tests passed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validity = CheckValidity(spark,\"/output_stg/immi_fact.parquet\",\"/output_stg/demog_dim.parquet\",\"/output_stg/airport_dim.parquet\")\n",
    "validity.check_demog_dim()\n",
    "validity.check_airport_dim()\n",
    "validity.check_fact_table()\n",
    "validity.check_integrity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Please refer to the Data_Dictionary.md file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* I used Apache Spark for ETL process because we can load the Petabytes of data and can process it without any hassle by setting up multi-node clusters.\n",
    "* Data needs to be updated every month since immigration fact table is partitioned by month. However if required we can also have weekly granularity, we can partition it weekly and hence update it respectively.\n",
    "* I would use the following approch under the following scenarios:  \n",
    "    * <b>The data was increased by 100x</b> - I would use AWS + Spark to store input sources and output sources in s3 buckets\n",
    "      along with appropriately partitoned columns   \n",
    "    * <b>The data populates a dashboard that must be updated on a daily basis by 7am every day</b> - I would use AWS + Spark + \n",
    "      Airflow to run the ETL job by 7 am everyday to visualize ETL workflows and debug when necessary.\n",
    "    * <b>The database needed to be accessed by 100+ people</b> - I would use AWS + Spark + Airflow and store output tables in \n",
    "      Redshift so that 100+ people can run/access Analytical queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
